{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca10f119-9ec7-4a2c-b7aa-6fb41f6e160e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %load_ext autotime|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360badf8-1a72-4e2f-bd84-80487727cbdd",
   "metadata": {},
   "source": [
    "## This notebook was used to collect the instruct_qa dataset into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6108f6a-c081-4563-bbad-373b9337343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['alpaca-7b', 'flan-t5-xxl', 'gpt-3.5-turbo', 'llama-2-7b-chat']\n",
    "datasets = ['hotpot_qa', 'natural_questions', 'topiocqa']\n",
    "PATH_NAME = \"\"   # you need to change it to with the path where you keep instruct qa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71e47dba-0bee-4c91-afe8-52ddf185fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "\n",
    "column_names = ['id', 'dataset', 'model_name', 'question', 'answer', 'ground_truth', 'context', 'human_verdict']\n",
    "df = pd.DataFrame(columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef0e97c-c7a1-4c8e-95f1-56ba6f661b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "data_collector = []\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    for name in model_names: \n",
    "        \n",
    "        \n",
    "        files = os.listdir(os.path.join(PATH_NAME, 'results', dataset, 'response'))\n",
    "        \n",
    "        # print(files)\n",
    "        \n",
    "        pattern = f\"^{dataset}_validation_c.*{name}_r-gold-passage_prompt.*qa_p-0\\.95_t-0\\.95_s-0\\.jsonl$\"\n",
    "        \n",
    "        # print(pattern)\n",
    "        \n",
    "        selected_file=None\n",
    "        \n",
    "        for file in files: \n",
    "            if re.match(pattern, file): \n",
    "                print(file)\n",
    "                selected_file = file\n",
    "                break\n",
    "     \n",
    "        # break\n",
    "        \n",
    "        path_answers = os.path.join(PATH_NAME, 'results', dataset, 'response',selected_file)\n",
    "        path_evaluation = os.path.join(PATH_NAME, 'human_eval_annotations', 'correctness', dataset, f\"{name}_human_eval_results.json\")\n",
    "        \n",
    "        print(path_answers) \n",
    "        print(path_evaluation)\n",
    "        print(20*\"=\")\n",
    "        \n",
    "        answers = []\n",
    "\n",
    "        with open(path_answers, \"r\") as f:     \n",
    "            for line in f: \n",
    "                answers.append(json.loads(line)) \n",
    "\n",
    "        # print(answers)\n",
    "\n",
    "        with open(path_evaluation, \"r\") as f: \n",
    "            annotations = json.load(f) \n",
    "\n",
    "\n",
    "        for answer in answers: \n",
    "            # print(answer.keys())\n",
    "            id_ = answer['id_'] \n",
    "            question = answer['question'] \n",
    "            llm_answer = answer['response'] \n",
    "            ground_truth = '/'.join(a for a in answer['answer']) \n",
    "            prompt = answer['prompt'] \n",
    "            verdict = None \n",
    "\n",
    "\n",
    "            for evaluated_answer in annotations:\n",
    "                # print(evaluated_answer.keys())\n",
    "                # break\n",
    "                if int(id_) == int(evaluated_answer['id_']):\n",
    "                    \n",
    "                    if int(id_) == 735: \n",
    "                        print(evaluated_answer)\n",
    "                    \n",
    "                    verdict=evaluated_answer['annotation'] \n",
    "                    \n",
    "\n",
    "                    data_collector.append({\n",
    "                        \"id\" : int(id_), \n",
    "                        \"model_name\" : name, \n",
    "                        \"dataset\": dataset,\n",
    "                        \"question\": question, \n",
    "                        \"answer\": llm_answer, \n",
    "                        \"ground_truth\": ground_truth, \n",
    "                        \"prompt\": prompt, \n",
    "                        \"human_verdict\": verdict\n",
    "                    })\n",
    "\n",
    "                    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241b59a6-6195-4ab2-a2f3-640251241921",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_collector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b811676-9463-492d-9400-0ce84bac3b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_collector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "998c8681-ddc3-460a-82ad-265500b7de6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_name</th>\n",
       "      <th>dataset</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>prompt</th>\n",
       "      <th>human_verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>alpaca-7b</td>\n",
       "      <td>hotpot_qa</td>\n",
       "      <td>who is the younger brother of The episode gues...</td>\n",
       "      <td>Brian Doyle-Murray is the younger brother of t...</td>\n",
       "      <td>Bill Murray</td>\n",
       "      <td>Please answer the following question given the...</td>\n",
       "      <td>incorrect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263</td>\n",
       "      <td>alpaca-7b</td>\n",
       "      <td>hotpot_qa</td>\n",
       "      <td>Which character does this protagonist, who sec...</td>\n",
       "      <td>Romeo.</td>\n",
       "      <td>Tybalt</td>\n",
       "      <td>Please answer the following question given the...</td>\n",
       "      <td>incorrect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>279</td>\n",
       "      <td>alpaca-7b</td>\n",
       "      <td>hotpot_qa</td>\n",
       "      <td>What type of magazine is Eugene Habecker the c...</td>\n",
       "      <td>Christianity Today magazine.</td>\n",
       "      <td>evangelical Christian periodical</td>\n",
       "      <td>Please answer the following question given the...</td>\n",
       "      <td>incorrect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>317</td>\n",
       "      <td>alpaca-7b</td>\n",
       "      <td>hotpot_qa</td>\n",
       "      <td>What Kentucky county has a population of 60,31...</td>\n",
       "      <td>Oldham County.</td>\n",
       "      <td>Oldham County</td>\n",
       "      <td>Please answer the following question given the...</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>335</td>\n",
       "      <td>alpaca-7b</td>\n",
       "      <td>hotpot_qa</td>\n",
       "      <td>Mamie Gummer played the role of Nancy Crozier ...</td>\n",
       "      <td>The Good Wife (CBS)</td>\n",
       "      <td>CBS</td>\n",
       "      <td>Please answer the following question given the...</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id model_name    dataset  \\\n",
       "0   68  alpaca-7b  hotpot_qa   \n",
       "1  263  alpaca-7b  hotpot_qa   \n",
       "2  279  alpaca-7b  hotpot_qa   \n",
       "3  317  alpaca-7b  hotpot_qa   \n",
       "4  335  alpaca-7b  hotpot_qa   \n",
       "\n",
       "                                            question  \\\n",
       "0  who is the younger brother of The episode gues...   \n",
       "1  Which character does this protagonist, who sec...   \n",
       "2  What type of magazine is Eugene Habecker the c...   \n",
       "3  What Kentucky county has a population of 60,31...   \n",
       "4  Mamie Gummer played the role of Nancy Crozier ...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Brian Doyle-Murray is the younger brother of t...   \n",
       "1                                             Romeo.   \n",
       "2                       Christianity Today magazine.   \n",
       "3                                     Oldham County.   \n",
       "4                                The Good Wife (CBS)   \n",
       "\n",
       "                       ground_truth  \\\n",
       "0                       Bill Murray   \n",
       "1                            Tybalt   \n",
       "2  evangelical Christian periodical   \n",
       "3                     Oldham County   \n",
       "4                               CBS   \n",
       "\n",
       "                                              prompt human_verdict  \n",
       "0  Please answer the following question given the...     incorrect  \n",
       "1  Please answer the following question given the...     incorrect  \n",
       "2  Please answer the following question given the...     incorrect  \n",
       "3  Please answer the following question given the...       correct  \n",
       "4  Please answer the following question given the...       correct  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bec95070-d471-4d2d-a816-73dc848f7fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_name</th>\n",
       "      <th>dataset</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>prompt</th>\n",
       "      <th>human_verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>735</td>\n",
       "      <td>alpaca-7b</td>\n",
       "      <td>hotpot_qa</td>\n",
       "      <td>Are both Disney Magazine and The Ring still in...</td>\n",
       "      <td>Yes, both Disney Magazine and The Ring are sti...</td>\n",
       "      <td>no</td>\n",
       "      <td>Please answer the following question given the...</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>735</td>\n",
       "      <td>flan-t5-xxl</td>\n",
       "      <td>hotpot_qa</td>\n",
       "      <td>Are both Disney Magazine and The Ring still in...</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Please answer the following question given the...</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>735</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>hotpot_qa</td>\n",
       "      <td>Are both Disney Magazine and The Ring still in...</td>\n",
       "      <td>No, Disney Magazine ceased publication in Apri...</td>\n",
       "      <td>no</td>\n",
       "      <td>Please answer the following question given the...</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>735</td>\n",
       "      <td>llama-2-7b-chat</td>\n",
       "      <td>hotpot_qa</td>\n",
       "      <td>Are both Disney Magazine and The Ring still in...</td>\n",
       "      <td>No, both Disney Magazine and The Ring are no l...</td>\n",
       "      <td>no</td>\n",
       "      <td>[INST] &lt;&lt;SYS&gt;&gt;\\nPlease answer the following qu...</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id       model_name    dataset  \\\n",
       "8    735        alpaca-7b  hotpot_qa   \n",
       "108  735      flan-t5-xxl  hotpot_qa   \n",
       "208  735    gpt-3.5-turbo  hotpot_qa   \n",
       "308  735  llama-2-7b-chat  hotpot_qa   \n",
       "\n",
       "                                              question  \\\n",
       "8    Are both Disney Magazine and The Ring still in...   \n",
       "108  Are both Disney Magazine and The Ring still in...   \n",
       "208  Are both Disney Magazine and The Ring still in...   \n",
       "308  Are both Disney Magazine and The Ring still in...   \n",
       "\n",
       "                                                answer ground_truth  \\\n",
       "8    Yes, both Disney Magazine and The Ring are sti...           no   \n",
       "108                                                 no           no   \n",
       "208  No, Disney Magazine ceased publication in Apri...           no   \n",
       "308  No, both Disney Magazine and The Ring are no l...           no   \n",
       "\n",
       "                                                prompt human_verdict  \n",
       "8    Please answer the following question given the...       correct  \n",
       "108  Please answer the following question given the...       correct  \n",
       "208  Please answer the following question given the...       correct  \n",
       "308  [INST] <<SYS>>\\nPlease answer the following qu...       correct  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['id'] == 735]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89784627-4130-47a5-a006-d01737ca24bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('instruct_qa_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee94780-709d-4c7e-b2f6-6e040f62957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dataset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e27e97-823d-458f-ac82-b9a490eb9dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['dataset'] == 'hotpot_qa']['human_verdict'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628de005-0e69-4709-b451-8d89291ed756",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['dataset'] == 'natural_questions']['human_verdict'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8128aae8-daab-4c44-9bfb-648a1b02e12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['dataset'] == 'topiocqa']['human_verdict'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4ca9f43-4fdf-4ba6-95ca-f990618c247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['answer'] = df['answer'].fillna('No answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b33759c-c9e5-48f7-ab87-1fac1a2fcd2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50e0f503-5fe9-4d41-ab51-b1bb33c3d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('instruct_qa_full.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
